#+TITLE: the Lifetime of an IO Request 
* An Example
#+BEGIN_QUOTE
(root): entry_SYSCALL_64_after_hwframe
|
\-entry_SYSCALL_64_after_hwframe
 |
 \-do_syscall_64
  |
  \-__x64_sys_ioctl
   |
   \-sg_ioctl
    |
    \-sg_ioctl_common
     |
     \-sg_new_write.isra.0
      |
      \-sg_common_write.isra.0
       |
       | 在 sg_common_write 到 blk_mq_alloc_request 之间
       | 这个调用被省略了两层调用：
       | sg_common_write -> sg_start_req -> blk_get_request -> blk_mq_alloc_request
       |
       | 传入了一个 request_queue, OP 是 REQ_OP_DRV_OUT : REQ_OP_DRV_IN, flag 是 0
       | 
       | blk_mq_alloc_request:
       | 1. 创建一个 blk_mq_alloc_data 结构；
       | 2. blk_queue_enter: 增加 q->q_usage_counter 计数；
       | 3. __blk_mq_alloc_request 这是真正 alloc request 的地方，稍候展开谈这个函数；
       | 4. 补完 rq (设置 __data_len, __sector, bio, biotail 等 field), 当然我不是很明白
       |    为什么不在 __blk_queue_request 里面一起做了？
       |
       | __blk_mq_alloc_request 做的事情比较多：
       | 1. blk_queue_rq_alloc_time 检查 q 的 queue_flags 是否设置了 QUEUE_FLAG_RQ_ALLOC_TIME；
       |    这会决定是否设置 alloc_time_ns， alloc_time_ns 表示 reqeust 创建的时间。
       | 2. 处理 elevator 一些复杂情况；这部分没有看懂
       | 3. 取软队列(blk_mq_get_ctx)和硬件队列(blk_mq_map_queue)，需要说明 blk_mq_ctx 和
       |    blk_mq_hw_ctx 的意义。
       | 4. blk_mq_get_tag, 这个时候，data 的 flag 加上了 BLK_MQ_REQ_NOWAIT ，已经取得软硬队列。
       |    tags 是一个 bitmap，主要特别之处在与分了不同的 word 的，这样上锁方便一点，另外还可以
       |    resize 。它会记录整个硬件队列深度，一旦满了，无法取得 tag. 队列深度是 block device 
       |    决定的。
       | 5. blk_mq_rq_ctx_init, 正式填充 struct request, 主要要开 CONFIG_BLK_RQ_ALLOC_TIME 
       |    才会记录 alloc_time_ns，并且在这个时候填充 start_time_ns (需要 alloc_time_ns).
       |    硬件队列增加1.
       \-kretprobe_trampoline(blk_mq_alloc_request)
       |  
       |  还在 sg_start_req 中
       |  blk_rq_map_user_iov: 把缓存数据封装成 bio 加入 request 里面。
       |  调用 blk_rq_append_bio.
       |
       \-blk_rq_map_user_iov
       |
       |  此处应该在 blk_execute_rq_nowait 中，但是不知道为什么少了这个调用层次。
       |  这个函数赋值了 rq->part (block_device 的指针)，update_io_ticks 可能不会有写入。
       |
       \-blk_account_io_start
       |
       |  有点奇怪，进入 blk_mq_sched_insert_request 
       |  没有抓到调用 __blk_mq_insert_request ，估计是走了
       |  elevator 的 insert_requests
       |
       \-blk_mq_sched_insert_request
        |
	|  什么是 SRCU-protected structure?
	|  这里为什么有两种锁？
	|  上锁->判断是否 need_run->解锁-> if need_run then run
	|  need_run 的判断条件：
	|  !blk_queue_quiesced && blk_mq_hctx_has_pending
	|  后面一个条件比较好了解，但是前面一个需要查一下什么时候，
	|  队列会被 quiesced (本质上就是设置一个 bit)。
	|
        \-blk_mq_run_hw_queue
         |
	 |  什么时候 blk_mq_hctx_stopped?
	 |  非 async 请求且 hctx 没有 blocking 都直接 get_cpu()
	 |  然后走 __blk_mq_run_hw_queue()
	 |  但凡这里走不同，都走延时队列。
	 |  hctx->run_work 在哪里定义？
	 |
         \-__blk_mq_delay_run_hw_queue
          |  
	  |  很单纯的函数，如果 hctx->flags & BLK_MQ_F_BLOCKING
	  |  会标注 might_sleep()
	  |  然后就是上锁，dispatch_requests.
	  |
          \-__blk_mq_run_hw_queue
           |
	   |  这里有意思的是，如果 __blk_mq_sched_dispatch_requests 
	   |  返回了 EAGAIN，那么会重入 blk_mq_run_hw_queue.
	   |
           \-blk_mq_sched_dispatch_requests
            |
            \-__blk_mq_sched_dispatch_requests
             |                                                              
             | __blk_mq_sched_dispatch_requests -> blk_mq_do_dispatch_sched 
             | -> __blk_mq_do_dispatch_sched -> blk_mq_get_driver_tag       
	     | 
             |                                                              
             \-blk_mq_get_driver_tag
             |
             \-blk_mq_dispatch_rq_list
              |
              \-blk_mq_start_request
              |
	      |  blk_mq_rq_to_pdu 这个函数的逻辑我是真的看不懂。
	      |  
              \-scsi_queue_rq
	       |
               |  这个函数是在 scsi_queue_rq 中，在 scsi_dispatch_cmd 之前
	       |  的 blk_mq_start_request 里面被调用。
	       |  
               \-blk_add_timer
      |
      \-blk_rq_map_user
       |
       \-blk_rq_append_bio
      |
      \-blk_execute_rq_nowait
       |
       \-blk_mq_request_bypass_insert

(root): secondary_startup_64_no_verify
|
\-secondary_startup_64_no_verify
 |
 \-start_secondary
  |
  \-cpu_startup_entry
   |
   \-do_idle
    |
    \-flush_smp_call_function_from_idle
     |
     \-do_softirq
      |
      \-__softirqentry_text_start
       |  
       |  在 blk_mq_init 里面用 open_softirq 注册在
       |  BLOCK_SOFTIRQ 这个中断里面。所以我们要找找哪个函数
       |  raise 这个中断，这样我们可以知道上半是在哪里发出的
       |
       \-blk_done_softirq
        |  
	|  这个函数接收每个 cpu 上的 blk_cpu_done list
	|  跟 scsi 那篇文档印证了。
	|  遍历列表上每个 request ，然后调用 mq_ops->complete 
	|
        \-blk_complete_reqs
         |
	 |  在我们这个例子里面 complete 就是 scsi_complete.
	 |  scsi_decide_disposition: 这个函数其实就是读 cmd 里面的 flag.
	 |  如果返回的 disposition 是 SUCCESS，那么就调用 scsi_finish_command
	 |  我们看看两种插入队列的情况以及调用 scsi_eh_scmd_add.
	 |
	 |  scsi_queue_insert 有两个参数，一个是需要重新排队的 cmd, 另一个是需要重排的 reason
	 |  reason 决定了最后是 host 还是 target block. 这个函数最后会调用 blk_mq_requeue_request(插到哪个队列里面？)
	 |
	 |  如果出现其他错误，会调用 scsi_eh_scmd_added
	 |  这个函数首先会把 host 的状态设置为 SHOST_RECOVERY. 然后调用 scsi_eh_reset 
	 |  再将入参 scmd 中的 eh_entry 加到 host 的 eh_cmd_q 上。
	 |  最后会进行 call_rcu 这一步久完全不知道干了什么，好像只是增加了计数？
	 |
         \-scsi_complete
          |
	  |  这个函数首先会 scsi host/target/device 的 blocked 设置为0，然后调用 scsi_io_completion.
	  |
          \-scsi_finish_command
           |
	   |  
           \-scsi_io_completion
            |
	    |  1. blk_update_request: 这个函数会接收已经处理的字节数（bytes）,
	    |     然后合集 request 里面的 bio 总字节数跟这个已处理字节数是否相等，如果不峡谷你等，
	    |     说明还有需要处理的 bio， 后续处理在 scsi_io_completion 里面。
	    |  2. add_disk_randomness: 不明。
	    |  3. destroy_rcu_head: 也是不明白这个 cmd->rcu 到底是什么。
	    |  4. scsi_mq_uninit_cmd: 释放资源，但是里面有一个动态的 uninit_command 不清楚具体释放了什么。
	    |  5. scsi_run_queue_async: 这里又跑了异步任务和 blk_mq_run_hw_queues?
	    |
            \-scsi_end_request
             |
	     \- __blk_mq_end_request
	      |
              \-blk_stat_add
              |
              \-blk_account_io_done
              |
              \-sg_rq_end_io
               |
               \-blk_put_request
                |
                \-blk_mq_free_request
#+END_QUOTE
